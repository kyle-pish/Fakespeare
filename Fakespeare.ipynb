{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How does the literary and language structure of Shakespearean text influence the effectiveness of an LLM to generate authentic Shakespearean text?\n",
        "\n",
        "Can it generate text that resembles Shakespeares original works by picking up on typical Shakespearan themes, language, and flow?"
      ],
      "metadata": {
        "id": "6c2RpXJiSC3c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vI3RrUhEbBi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQHxDym-EfL0"
      },
      "outputs": [],
      "source": [
        "# Load Shakespeare text\n",
        "\n",
        "path_to_file = \"train_shakespeare.txt\"\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3hEKY7hEjJc"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKoPfibnEniR"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                             return_sequences=True,\n",
        "                             stateful=True,\n",
        "                             recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsgAUyFhEwKA"
      },
      "outputs": [],
      "source": [
        "# Function to generate text\n",
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 100000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oFhrnJcFYPt"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdWKmX7aFb7o"
      },
      "outputs": [],
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "# Checkpoint directory\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Training the model\n",
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMuUu95rDqYk"
      },
      "outputs": [],
      "source": [
        "# Restore the latest checkpoint\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "# Generate text starting from 'ROMEO: '\n",
        "print(generate_text('training_checkpoints/ckpt_30.data-00000-of-00001', start_string=u\"ROMEO: \"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Check if GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "# Load Shakespeare text\n",
        "path_to_file = \"train_shakespeare.txt\"\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                             return_sequences=True,\n",
        "                             stateful=True,\n",
        "                             recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 10000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 100000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "# Check if GPU is available\n",
        "if len(tf.config.experimental.list_physical_devices('GPU')) > 0:\n",
        "    print(\"GPU is available. Training on GPU...\")\n",
        "    with tf.device('/GPU:0'):\n",
        "        model = build_model(\n",
        "            vocab_size=len(vocab),\n",
        "            embedding_dim=embedding_dim,\n",
        "            rnn_units=rnn_units,\n",
        "            batch_size=BATCH_SIZE)\n",
        "\n",
        "        # Checkpoint directory\n",
        "        checkpoint_dir = './training_checkpoints'\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_prefix,\n",
        "            save_weights_only=True)\n",
        "\n",
        "        # Compiling the model\n",
        "        model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "        # Training the model\n",
        "        EPOCHS = 200\n",
        "\n",
        "        history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "        # Restore the latest checkpoint\n",
        "        model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "        model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "        model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "        # Generate text starting from 'ROMEO: '\n",
        "        print(generate_text(model, start_string=u\"ROMEO: \"))\n",
        "else:\n",
        "    print(\"GPU not available. Training on CPU...\")\n",
        "    model = build_model(\n",
        "        vocab_size=len(vocab),\n",
        "        embedding_dim=embedding_dim,\n",
        "        rnn_units=rnn_units,\n",
        "        batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Checkpoint directory\n",
        "    checkpoint_dir = './training_checkpoints'\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_prefix,\n",
        "        save_weights_only=True)\n",
        "\n",
        "    # Compiling the model\n",
        "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "    # Training the model\n",
        "    EPOCHS = 100\n",
        "\n",
        "    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "    # Restore the latest checkpoint\n",
        "    model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "    model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "    # Generate text starting from 'ROMEO: '\n",
        "    print(generate_text(model, start_string=u\"ROMEO: \"))\n"
      ],
      "metadata": {
        "id": "odTVv3hMHPyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis**"
      ],
      "metadata": {
        "id": "IN3rRUDZNLZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Language and Vocabulary:** Shakespeare is known for his extensive vocabulary and creative use of language. The generated text includes words and phrases that seem to fit the Shakespeare style such as Ahonord, passado, and bellows-mender, which evoke the language of Shakespearean plays.\n",
        "Rhythm and Meter: Shakespeare often wrote in iambic pentameter, a rhythmic pattern of stressed and unstressed syllables. While the generated text doesn't exactly showcase this meter, it contains rhythmic elements similar to that of Shakespeare's work.\n",
        "\n",
        "\n",
        "**Characters and Themes:** The text contains classic characters from the works of Shakespeare and they all seem to have the same character traits and personalities as they do in real Shakespeare. Characters such as Romeo, Hamlet, and Macbeth constantly mention themes of death, love, power, or make sly sexual innuendos.\n",
        "\n",
        "    “in the morning’s eyeing blessed my love’s tongue, bring him silently”\n",
        "\n",
        "    “Here’s my father and rest, but all is death, but body’s bones.”\n",
        "\n",
        "    “I will not stay in the siege of loving death.\n",
        "    But let the mind to you; farewell. Now I am alone.”\n",
        "\n",
        "    “They bleed so that I may love thee.”\n",
        "\n",
        "    “And Tybalt’s dead”\n",
        "\n",
        "    “Sir, in my heart I am an oppressor”\n",
        "\n",
        "    “Come, sir, There’s blood upon your brow”\n",
        "\n",
        "    “I should live to be born. my lips that lie look upon within.”\n",
        "\n",
        "    “I had most need? The Queen and his mouth, his finespire, abound for if I can see”\n",
        "\n",
        "**Dramatic Elements:** Shakespearean plays are known for their dramatic aspects. While not fully included in this, the generated text contains elements of drama such as fights. One spot I particularly enjoyed because it made me chuckle is when a fight spontaneously broke out.\n",
        "\n",
        "    NURSE:\n",
        "    She sees, my lord, and I will wear it.\n",
        "\n",
        "    HAMLET:\n",
        "    I am but merry!—\n",
        "    I’ll call up our wisest fair Jepantasion.\n",
        "\n",
        "    POLONIUS:\n",
        "    Have I, And put it to you.\n",
        "\n",
        "    ROMEO:\n",
        "    Come, sir, your passado.\n",
        "\n",
        "    [They fight.]\n",
        "\n",
        "Not only did I find this part particularly funny, but I think it also showcases some of the spontaneity of Shakespeare's writing, as sometimes when reading Shakespeare you’re on the edge of your seat, not always knowing what to expect next.\n",
        "\n",
        "**In summary**, while the generated text may not achieve the exact mastery and skill of Shakespeare's writing, it captures enough elements of his style to evoke a sense of familiarity and with his works. In my opinion this highlights Shakespeare's uniqueness and influence of his works, which continue to inspire and influence writers centuries later.\n",
        "\n",
        "**Honorable mentions** of lines that made me laugh\n",
        "\n",
        "    “So let him dumb head”\n",
        "\n",
        "    “FIRST CLOWN:\n",
        "    [Sings.]\n",
        "    An old maid’s, or their own distracted groves,\n",
        "    The virtue of the thure, Lords, Or if thou know’st mine eyes,\n",
        "    And braggart with my tongue!—But, gentle sweet, you shall hear, go join you, I’ll fa you. Do you notice men?,\n",
        "    I qual night, and left me with rum to say, I saw the other senses,\n",
        "    Or my true knight!”\n",
        "\n",
        "**Please note this results were from my specific generation, your generations may produce varying results**"
      ],
      "metadata": {
        "id": "vULxBLeOL8oz"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}