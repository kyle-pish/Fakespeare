{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How does the literary and language structure of Shakespearean text influence the effectiveness of an LLM to generate authentic Shakespearean text?\n",
        "\n",
        "Can it generate text that resembles Shakespeares original works by picking up on typical Shakespearan themes, language, and flow?"
      ],
      "metadata": {
        "id": "6c2RpXJiSC3c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vI3RrUhEbBi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQHxDym-EfL0",
        "outputId": "92129d79-9148-42f8-b992-0d9bc8b31d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 521800 characters\n",
            "77 unique characters\n"
          ]
        }
      ],
      "source": [
        "# Load Shakespeare text\n",
        "\n",
        "path_to_file = \"train_shakespeare.txt\"\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3hEKY7hEjJc"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKoPfibnEniR"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                             return_sequences=True,\n",
        "                             stateful=True,\n",
        "                             recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsgAUyFhEwKA"
      },
      "outputs": [],
      "source": [
        "# Function to generate text\n",
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 100000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oFhrnJcFYPt"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "fdWKmX7aFb7o",
        "outputId": "8566f9ce-1f51-4841-89c4-6c8ee3718b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "29/80 [=========>....................] - ETA: 4:26 - loss: 3.4327"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-8e8c013ffae6>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = build_model(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "# Checkpoint directory\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Training the model\n",
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMuUu95rDqYk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "94ac7ee6-1680-4c2f-f6ce-79edd61f9528"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'reset_states'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-4889d780788a>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Generate text starting from 'ROMEO: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_checkpoints/ckpt_30.data-00000-of-00001'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu\"ROMEO: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-03f5b1e3cc9d>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Here batch size == 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'reset_states'"
          ]
        }
      ],
      "source": [
        "# Restore the latest checkpoint\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "# Generate text starting from 'ROMEO: '\n",
        "print(generate_text('training_checkpoints/ckpt_30.data-00000-of-00001', start_string=u\"ROMEO: \"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Check if GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "# Load Shakespeare text\n",
        "path_to_file = \"train_shakespeare.txt\"\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.LSTM(rnn_units,\n",
        "                             return_sequences=True,\n",
        "                             stateful=True,\n",
        "                             recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(model, start_string):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "\n",
        "    # Number of characters to generate\n",
        "    num_generate = 10000\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperatures results in more predictable text.\n",
        "    # Higher temperatures results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = 1.0\n",
        "\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        # remove the batch dimension\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        # We pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return (start_string + ''.join(text_generated))\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 100000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "# Check if GPU is available\n",
        "if len(tf.config.experimental.list_physical_devices('GPU')) > 0:\n",
        "    print(\"GPU is available. Training on GPU...\")\n",
        "    with tf.device('/GPU:0'):\n",
        "        model = build_model(\n",
        "            vocab_size=len(vocab),\n",
        "            embedding_dim=embedding_dim,\n",
        "            rnn_units=rnn_units,\n",
        "            batch_size=BATCH_SIZE)\n",
        "\n",
        "        # Checkpoint directory\n",
        "        checkpoint_dir = './training_checkpoints'\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=checkpoint_prefix,\n",
        "            save_weights_only=True)\n",
        "\n",
        "        # Compiling the model\n",
        "        model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "        # Training the model\n",
        "        EPOCHS = 200\n",
        "\n",
        "        history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "        # Restore the latest checkpoint\n",
        "        model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "        model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "        model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "        # Generate text starting from 'ROMEO: '\n",
        "        print(generate_text(model, start_string=u\"ROMEO: \"))\n",
        "else:\n",
        "    print(\"GPU not available. Training on CPU...\")\n",
        "    model = build_model(\n",
        "        vocab_size=len(vocab),\n",
        "        embedding_dim=embedding_dim,\n",
        "        rnn_units=rnn_units,\n",
        "        batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Checkpoint directory\n",
        "    checkpoint_dir = './training_checkpoints'\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_prefix,\n",
        "        save_weights_only=True)\n",
        "\n",
        "    # Compiling the model\n",
        "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "    # Training the model\n",
        "    EPOCHS = 100\n",
        "\n",
        "    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "    # Restore the latest checkpoint\n",
        "    model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "    model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "    # Generate text starting from 'ROMEO: '\n",
        "    print(generate_text(model, start_string=u\"ROMEO: \"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odTVv3hMHPyL",
        "outputId": "fcfb3977-2d41-4b60-bb2b-f57d61ae895e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "Length of text: 521800 characters\n",
            "77 unique characters\n",
            "GPU is available. Training on GPU...\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._iterations\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161/161 [==============================] - 5s 19ms/step - loss: 2.6765\n",
            "Epoch 2/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 1.9725\n",
            "Epoch 3/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 1.7270\n",
            "Epoch 4/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 1.5846\n",
            "Epoch 5/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 1.4903\n",
            "Epoch 6/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 1.4180\n",
            "Epoch 7/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 1.3585\n",
            "Epoch 8/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 1.3037\n",
            "Epoch 9/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 1.2494\n",
            "Epoch 10/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 1.1953\n",
            "Epoch 11/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 1.1378\n",
            "Epoch 12/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 1.0777\n",
            "Epoch 13/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 1.0148\n",
            "Epoch 14/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.9481\n",
            "Epoch 15/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.8834\n",
            "Epoch 16/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.8191\n",
            "Epoch 17/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.7554\n",
            "Epoch 18/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.6991\n",
            "Epoch 19/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6457\n",
            "Epoch 20/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.5979\n",
            "Epoch 21/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.5547\n",
            "Epoch 22/200\n",
            "161/161 [==============================] - 3s 16ms/step - loss: 0.5196\n",
            "Epoch 23/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.4859\n",
            "Epoch 24/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4612\n",
            "Epoch 25/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4375\n",
            "Epoch 26/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4168\n",
            "Epoch 27/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3994\n",
            "Epoch 28/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3840\n",
            "Epoch 29/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3745\n",
            "Epoch 30/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.3634\n",
            "Epoch 31/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3553\n",
            "Epoch 32/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3479\n",
            "Epoch 33/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3413\n",
            "Epoch 34/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.3354\n",
            "Epoch 35/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3303\n",
            "Epoch 36/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3250\n",
            "Epoch 37/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3224\n",
            "Epoch 38/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3178\n",
            "Epoch 39/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3121\n",
            "Epoch 40/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3102\n",
            "Epoch 41/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3094\n",
            "Epoch 42/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.3057\n",
            "Epoch 43/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3018\n",
            "Epoch 44/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2993\n",
            "Epoch 45/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2985\n",
            "Epoch 46/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.2972\n",
            "Epoch 47/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.2981\n",
            "Epoch 48/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2953\n",
            "Epoch 49/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2925\n",
            "Epoch 50/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.2903\n",
            "Epoch 51/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2874\n",
            "Epoch 52/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2878\n",
            "Epoch 53/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.2870\n",
            "Epoch 54/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2850\n",
            "Epoch 55/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2820\n",
            "Epoch 56/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2850\n",
            "Epoch 57/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2809\n",
            "Epoch 58/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.2838\n",
            "Epoch 59/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2827\n",
            "Epoch 60/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2804\n",
            "Epoch 61/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2799\n",
            "Epoch 62/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.2761\n",
            "Epoch 63/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2760\n",
            "Epoch 64/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2756\n",
            "Epoch 65/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2743\n",
            "Epoch 66/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2740\n",
            "Epoch 67/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2760\n",
            "Epoch 68/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2747\n",
            "Epoch 69/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2743\n",
            "Epoch 70/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2747\n",
            "Epoch 71/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2745\n",
            "Epoch 72/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2779\n",
            "Epoch 73/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2734\n",
            "Epoch 74/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2723\n",
            "Epoch 75/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2732\n",
            "Epoch 76/200\n",
            "161/161 [==============================] - 3s 16ms/step - loss: 0.2703\n",
            "Epoch 77/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2675\n",
            "Epoch 78/200\n",
            "161/161 [==============================] - 3s 15ms/step - loss: 0.2664\n",
            "Epoch 79/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2679\n",
            "Epoch 80/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2672\n",
            "Epoch 81/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2683\n",
            "Epoch 82/200\n",
            "161/161 [==============================] - 3s 16ms/step - loss: 0.2656\n",
            "Epoch 83/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2663\n",
            "Epoch 84/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2682\n",
            "Epoch 85/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2695\n",
            "Epoch 86/200\n",
            "161/161 [==============================] - 3s 16ms/step - loss: 0.2714\n",
            "Epoch 87/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2718\n",
            "Epoch 88/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2755\n",
            "Epoch 89/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2742\n",
            "Epoch 90/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2712\n",
            "Epoch 91/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2714\n",
            "Epoch 92/200\n",
            "161/161 [==============================] - 3s 16ms/step - loss: 0.2686\n",
            "Epoch 93/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2686\n",
            "Epoch 94/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2674\n",
            "Epoch 95/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2646\n",
            "Epoch 96/200\n",
            "161/161 [==============================] - 3s 16ms/step - loss: 0.2623\n",
            "Epoch 97/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2658\n",
            "Epoch 98/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2653\n",
            "Epoch 99/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2685\n",
            "Epoch 100/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2683\n",
            "Epoch 101/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2706\n",
            "Epoch 102/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2716\n",
            "Epoch 103/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2765\n",
            "Epoch 104/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2814\n",
            "Epoch 105/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2826\n",
            "Epoch 106/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2838\n",
            "Epoch 107/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2885\n",
            "Epoch 108/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2831\n",
            "Epoch 109/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2782\n",
            "Epoch 110/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2698\n",
            "Epoch 111/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2661\n",
            "Epoch 112/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2597\n",
            "Epoch 113/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2570\n",
            "Epoch 114/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2571\n",
            "Epoch 115/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2569\n",
            "Epoch 116/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2627\n",
            "Epoch 117/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2690\n",
            "Epoch 118/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2876\n",
            "Epoch 119/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3075\n",
            "Epoch 120/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3279\n",
            "Epoch 121/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3497\n",
            "Epoch 122/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3541\n",
            "Epoch 123/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3537\n",
            "Epoch 124/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3344\n",
            "Epoch 125/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3149\n",
            "Epoch 126/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2905\n",
            "Epoch 127/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2746\n",
            "Epoch 128/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2604\n",
            "Epoch 129/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2511\n",
            "Epoch 130/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2423\n",
            "Epoch 131/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2394\n",
            "Epoch 132/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2359\n",
            "Epoch 133/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2387\n",
            "Epoch 134/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2438\n",
            "Epoch 135/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2501\n",
            "Epoch 136/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2692\n",
            "Epoch 137/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3188\n",
            "Epoch 138/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4218\n",
            "Epoch 139/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.5286\n",
            "Epoch 140/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.5384\n",
            "Epoch 141/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.5098\n",
            "Epoch 142/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4618\n",
            "Epoch 143/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4214\n",
            "Epoch 144/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3847\n",
            "Epoch 145/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3583\n",
            "Epoch 146/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3330\n",
            "Epoch 147/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3131\n",
            "Epoch 148/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2922\n",
            "Epoch 149/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2825\n",
            "Epoch 150/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2722\n",
            "Epoch 151/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2649\n",
            "Epoch 152/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2585\n",
            "Epoch 153/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2554\n",
            "Epoch 154/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2585\n",
            "Epoch 155/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2628\n",
            "Epoch 156/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2740\n",
            "Epoch 157/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.2922\n",
            "Epoch 158/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.3583\n",
            "Epoch 159/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4968\n",
            "Epoch 160/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6393\n",
            "Epoch 161/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6687\n",
            "Epoch 162/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6479\n",
            "Epoch 163/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.5893\n",
            "Epoch 164/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.5514\n",
            "Epoch 165/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.5227\n",
            "Epoch 166/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4983\n",
            "Epoch 167/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4814\n",
            "Epoch 168/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4776\n",
            "Epoch 169/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4729\n",
            "Epoch 170/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4628\n",
            "Epoch 171/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4540\n",
            "Epoch 172/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4479\n",
            "Epoch 173/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4457\n",
            "Epoch 174/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4470\n",
            "Epoch 175/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4528\n",
            "Epoch 176/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4702\n",
            "Epoch 177/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.4941\n",
            "Epoch 178/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.5183\n",
            "Epoch 179/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.5602\n",
            "Epoch 180/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.5976\n",
            "Epoch 181/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6363\n",
            "Epoch 182/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6624\n",
            "Epoch 183/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6722\n",
            "Epoch 184/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6771\n",
            "Epoch 185/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6947\n",
            "Epoch 186/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6988\n",
            "Epoch 187/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6876\n",
            "Epoch 188/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.6931\n",
            "Epoch 189/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.7025\n",
            "Epoch 190/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.7296\n",
            "Epoch 191/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.7436\n",
            "Epoch 192/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.7645\n",
            "Epoch 193/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.7823\n",
            "Epoch 194/200\n",
            "161/161 [==============================] - 3s 16ms/step - loss: 0.8014\n",
            "Epoch 195/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.7893\n",
            "Epoch 196/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.7858\n",
            "Epoch 197/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.8093\n",
            "Epoch 198/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.8319\n",
            "Epoch 199/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.8599\n",
            "Epoch 200/200\n",
            "161/161 [==============================] - 3s 14ms/step - loss: 0.8549\n",
            "ROMEO: in the Palace\n",
            "That if you will know, Laertes, and your father o’ers. What a head was\n",
            "Tybalt’s dear come with Montague.\n",
            "Fellow the scrolang, which stains him;\n",
            "Thou art their beds, indifferce and so escepor.\n",
            "Spakes s father’s bunty\n",
            "That givy th’eff lost and fire shall me go inhourselord, to make them rack.\n",
            "Come, for him, lost, neven and patches doth seed\n",
            "The hateful slulO—ONIUS.\n",
            "This is one of my know. Untard thee,\n",
            "And coucher in sighs, and feek of payife\n",
            "To sold and curdents to me,\n",
            "And good night to have me distract. But, look you now!\n",
            "\n",
            "HORATIO.\n",
            "Leeves shall you again, I pry youth, sir, not I took with A.\n",
            "I chief-satitor art thou but be my many cooping.\n",
            "\n",
            "QUINCE.\n",
            "Thou hast shown a kind of love;\n",
            "That is something morn my nature; forgone?\n",
            "\n",
            " Enter Thristing.\n",
            "\n",
            "FIRST CLOWNo protence\n",
            "Near thou wilt to this Forre should. But tell him ept.\n",
            "Here is poor,\n",
            "Hold in his heart, hith faither be king,\n",
            "Where inkixon more is; Each was and is, or command\n",
            "This nighs in duin,\n",
            "Unworthy than scener’d by that name in thine it:\n",
            "To no me love out. What’s thou be not of be my one.\n",
            "\n",
            " [_Exeunt._]\n",
            "\n",
            "SCENE III. Friar Lawrence’s Cell’d.\n",
            "\n",
            "FIRST WITCH. Kings and Guildenstern.\n",
            "\n",
            "[_Exeunt RosencrantzAANSSED.\n",
            "Read me to the best.\n",
            "\n",
            "MACBETH.\n",
            "Come, thou art drawlt not let me in the powerford\n",
            "Was Tybalt’s double babe’t counsel. Or I remember when I note;\n",
            "In friends yocong moved loudest\n",
            "Till breaks of us. Theshiek, a most mine night,\n",
            "GENTLOWM.\n",
            "[_Aside._] Though thou and diggarter, imme the Palace e flattering lustness. I am thy Lord Help, help!\n",
            "\n",
            " Enter Dost and Snignz.\n",
            "Stays the grostleng book.\n",
            "\n",
            " Enter Capulet in Paris;\n",
            "And hath been the intenuals. Will you please; and where the maments, being ingxearly provide.\n",
            "My lord, I am at this think ’ speak of night;\n",
            "The starright hold their counsel of the Friar\n",
            "Shur sharp allow; this in comble,\n",
            "Where in Denmark, my you and so bridet.\n",
            "\n",
            "LYSANDER.\n",
            "I do beseech you.\n",
            "\n",
            "raming a while!\n",
            "\n",
            "LAERTES.\n",
            "[_Within._] My lord, well seize.\n",
            "\n",
            "LAERTES.\n",
            "O, I dread to the sere, Fledes and find orise.\n",
            "Take it not within my shot.\n",
            "\n",
            "DONALIA.\n",
            "I do not least unnusion of my herand, I will come dowed you your cause convince\n",
            "By the quacivy tainting think i’ th’ estern’s face, myself.\n",
            "\n",
            "FIRST CLOWN.\n",
            "Why, there the play\n",
            "Till him our knatesy man, fled Tybalt? Gives E.\n",
            "Marry, will I go; before van, and is sometime\n",
            "O’ who is the thing, werminatery of force,\n",
            "To brat lead the dog, of the piomed of my sin,\n",
            "And show the precion stars through thither ny\n",
            "Drink off that love pawdy.\n",
            "\n",
            "HAMLET.\n",
            "Amazemn, for England key\n",
            " as a smale-gress, my sison. Your is, reconcil’d love,\n",
            "And thou sabitainots; and ’tis not:\n",
            "This still hath leing me ept.\n",
            "And now I protencing. Do young men’s very sides, that I seel)\n",
            "That he has lightly, it were most implexhan and waking;\n",
            "Spakes shall all: and farewell light sharchy love\n",
            "nature-gyment ’gainstankinl, that if it is to me.\n",
            "But looke this visitated.\n",
            "I dare not assays her eyes.\n",
            "Hake Dungay, all that do was rose a widoner is it we do:\n",
            "Which is the god of my nature, I will be cheer.\n",
            "[_Sings._]\n",
            "   By this to your fan from a lover, that puts’d his line!\n",
            "[_Receoding within._]\n",
            "\n",
            "What, should I it, look you thirst so shame abread,\n",
            "Thee strange of these and most unrap,\n",
            "And we watched clink to infeece;\n",
            "My night, come bitter woman satiry w ke;\n",
            "No too slain amine.\n",
            "\n",
            "SECOND MURDERE.\n",
            "How came his youth not proud, and something fancress th’riour or in some would soul’d to you do me;\n",
            "And that my pall’d with the confure and snuff\n",
            "That I rest me. Let the foul tell then this place,\n",
            "All forty daylight-wark, my life is sea in knave,\n",
            "Prifardel.—Come, sleepire so ear\n",
            "Is lie virtue here, thou weeps on a Montague,\n",
            "The frief of men, in spit the Dane, and her peshety.\n",
            "Come, in a virtuous rash,\n",
            "Whom I, the sun to tell thee joyful last nightom’s wife,\n",
            "And he which her which ten times prove an ass.\n",
            "\n",
            "CAPULET.\n",
            "What secrets to have illfain’d wife ent the course for a runsyous and treblogs.\n",
            "             Come, my lord,\n",
            "a sea; till for a show.\n",
            "Not night, you know his fair, since that night shalt be too holy banquence,\n",
            "Let her come longer rook to heaven.\n",
            "O vow, you shall go ather did not lie asleep.\n",
            "\n",
            " [_Music._]\n",
            "\n",
            "HERMIA.\n",
            "Do not said, that purg’d your chamber?\n",
            "\n",
            "MACDUFF.\n",
            "Was if something is here to o’er\n",
            "too tatch do,\n",
            "But vit well do, girns him out, and conceived,\n",
            "Bewithder. Friend, look to tr’efso friends,\n",
            "And she sends me bottossing forteding? O pile amage not go\n",
            " assue with thee.\n",
            "\n",
            " [_Exit Lord._]\n",
            "\n",
            "LADY CAPULET.\n",
            "E, she’ll hone todaid now with lesters of a brother gone,\n",
            "No more thanewilds are men’s very men.\n",
            "Where day you lack the chann there! Flouty,\n",
            "    Able poor and lose, and reck o’er you his native her cords me.\n",
            "\n",
            "LORDO.\n",
            "I saw’t.\n",
            "\n",
            "ROSTIWAR.\n",
            "This day’s daughter gones in field.\n",
            "\n",
            " Spurter of thy vigious and probrowion’s issue,\n",
            "Whispers the counsels platting flow.\n",
            "\n",
            " [_Snee did the flose, who starts away and notlementy of him,\n",
            "Do not cound encreant I say, he moves men,\n",
            "By Sinon\n",
            "\n",
            "DEMEO.\n",
            "Come, what a sweet reason\n",
            "To other made upon we answer jealous;\n",
            "Her which of three hot and full of closes I love,\n",
            "When rind bosoms heaving hither eye,\n",
            "   Whings themselves, and shorely from your revele,\n",
            "His since a dogue, and virtue perfect, with theart high worth awnafuays; and never die\n",
            "A dream impessors; but I am what name I of a mony, give quiet of me.\n",
            "If you ol’d, wrose unto this world!\n",
            "\n",
            "KING.\n",
            "Good Laertes,\n",
            "If it have hand in it hath waeten day\n",
            "mah now: rancount is our inteem\n",
            "Would at a little which take upon me.\n",
            "I protest my heart, and I am go.\n",
            "\n",
            "LADY MACBETH.\n",
            "Poor soul!\n",
            "Marry, may be some walk his back again,\n",
            "And madaminear Banquo walk’d tonight,\n",
            "But shall like the herald of the night\n",
            "Than a more shalle ringley?\n",
            "\n",
            "SECONDER.\n",
            "The sood Thisbe.\n",
            "\n",
            " [_Exit Hamlet dances false\n",
            "Forth a thisbe, come in.\n",
            "\n",
            "[_Exit an at thee know,\n",
            "Take fruitless monst; whole earth bady’s\n",
            "lord.\n",
            "\n",
            "[_Exit._]\n",
            "\n",
            " SCENE VI. Dunsinane a ple.\n",
            "\n",
            " Enter Demetrius and Hamlet.\n",
            "\n",
            "FORTINBRAS.\n",
            "True quite know thy graviling of my prison he father;\n",
            "So lov’d thy boys is dead, by the harood of fair,\n",
            "And there must plight, unhall the court?\n",
            "\n",
            "SECOND MURDERER.\n",
            "Sirrance at him.\n",
            "\n",
            "LAERTES.\n",
            "Go, be not your longer.\n",
            "\n",
            "\n",
            "OPHELIA.\n",
            "What to gavive me so, young menter to happiert,\n",
            "Which can that you so ey most excellents are purposes finds.\n",
            "Now for this sight having one up,\n",
            "And should again thy eyes, which her news.\n",
            "Hie you this fellow him are; this scancelod so the\n",
            "wife.\n",
            "Now that is hour before some gone,\n",
            "To speak out to our warch, and hath been laughing center out, speak motio;\n",
            "And I may have it our state and furious Erring witfellopy in the vault,\n",
            "Most laught and rotemphar a little weary,\n",
            "For y the is truly. This love is gone? Come on,\n",
            "My trag, and unfections do when well welcome. For look\n",
            "you, good nor so will how tongueverous thoughts.\n",
            "\n",
            "JULIET.\n",
            "Love, I can convercome; ith give it what\n",
            "noinetules his disease, and could keep compare\n",
            "For see the dogs without thoughts:\n",
            "I’ll hath a heart there art the churchyard.\n",
            "\n",
            "FIRST MURDERE.\n",
            "Have you me lord?\n",
            "\n",
            "LAERTES.\n",
            "It shall forse be shark about the thit?\n",
            "\n",
            "ROMEO.\n",
            "Tell me dismy too: Welcome, gentle friends, screal heaven\n",
            "To where he loves you. Whisherro?\n",
            "\n",
            " [_Exit._]\n",
            "\n",
            "THESEUS.\n",
            "The grpase like starts, though power to ’scapes.\n",
            "A where you life shall not r,\n",
            "And ques up the bosom grief nups,\n",
            "But thou at I did, if to drierv’d of the duke.\n",
            "The from over provokes to seen;\n",
            "Then how hang something t with sick sworn,\n",
            "Shall I mort inua’ with a bushink)\n",
            "I had my dog like an legs.\n",
            "I sweet Danquo will have no tongue in your discold,\n",
            "But Dungrance arm from thy breath.\n",
            "\n",
            "LAERTES.\n",
            "Upon my lite stays heavy;\n",
            "But I beseech your sinser bags;\n",
            "For it that cossomach. Will it make thee know\n",
            "The murder of my mosthem thee.\n",
            "\n",
            "ROMEO.\n",
            "Is th’ at Macduff;\n",
            "And sut, we till her arm’sternain\n",
            "The father gave our honours, yours lathers, second murdershed, divine shadows speak;\n",
            "And oftealtu double.\n",
            "\n",
            "THIRD WITCH.\n",
            "Then you still my inge sore having now?\n",
            "\n",
            "HORATIO.\n",
            "Ay madam, died in heaven. His names are tomarried in my way again, let let me go.\n",
            "\n",
            "LADY MARDARIO.\n",
            "Thou art the owl Banquo king and there!\n",
            "\n",
            "SECOND CLOWN.\n",
            "What he could not, indeed Lysander from her?\n",
            "\n",
            "HORATIO.\n",
            "O God!\n",
            "\n",
            "KING.\n",
            "Lae hence to way.\n",
            "\n",
            "PYRAMUS.\n",
            "O he, a play in that stop and stark,\n",
            "qunner that he’s much be she mays, remain.\n",
            "\n",
            "MACDUFF.\n",
            "This stranger gut the door.\n",
            "\n",
            "NURSE.\n",
            "In Cupid’s sound.\n",
            "\n",
            "MALCOLM.\n",
            "’SS.\n",
            "Give them by rather clouds with strospants, els perture\n",
            "To anges the formship’d souls, and serve this present dear a complex.\n",
            "\n",
            "LENNOX.\n",
            "O Be leave your royal bed with josport,\n",
            "And yet, to bladievells to leave his saught.\n",
            "\n",
            "OBERON.\n",
            "Thre to berk you much, and do not this\n",
            "If et old thet to be crown’d\n",
            "So bloody thoughts sweep the serving time of huring hit was born in the world,\n",
            "Where hath grace and fears and blessity\n",
            "Than he is rewarded them.\n",
            "Let’s farewell.\n",
            "\n",
            "QUEEN.\n",
            "Norwhat hate commends: and think it nothing?\n",
            "What’s Montuge thee and a bless? This is too lasy,\n",
            "though in’t, to smy affair. This hath made milg bles from my sin\n",
            "Peasulos, and change time.\n",
            "\n",
            "MALCOLM.\n",
            "NTZ.\n",
            "My larkeun, here in that husbonciently tomorrow night. Boy, sir; for his no longs be think you, give these\n",
            "more humant to shake, so full of a belderfuldischarge.\n",
            "Lost stand, nor single, almost all coment;\n",
            "   I’ll are welconfine, and nobly surs._]\n",
            "\n",
            "JULIET.\n",
            "O find kill us not this tame red melancholy, all,\n",
            "Or lais defirde-ture. Farewell. O, lose youth!\n",
            "By your man gentleman?\n",
            "\n",
            "FIRST CLOWN.\n",
            "I tell her day the Queen strike me.\n",
            "\n",
            "SERVANT.\n",
            "I have this y“Since I can tell you now;\n",
            "Your blood of warman unner you, no?\n",
            "\n",
            "HAMLET.\n",
            "What, wice, without the of our reherb, that we shall have overwarle.\n",
            "Boy, one were it an untimely laid upon her broad.\n",
            "She’s dead, this marriagel very palmens\n",
            "Menter, are he breaks her shoet of hur.\n",
            "\n",
            "OPHELIA.\n",
            "You, Barquo.\n",
            "\n",
            "OPHELIA.\n",
            "He hath, have you since you do, i’ th’ night not o’ercharged,\n",
            "Who clost like singris we’ll dains\n",
            "That you mistake it. What should here in him in sads,\n",
            "Wasbs the visage of those from all contague.—\n",
            "The Queen and Pyrrhus not the nank,\n",
            "My will to her dagger end: but not poor,\n",
            "but thankfule before Thburchand a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis**"
      ],
      "metadata": {
        "id": "IN3rRUDZNLZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Language and Vocabulary:** Shakespeare is known for his extensive vocabulary and creative use of language. The generated text includes words and phrases that seem to fit the Shakespeare style such as Ahonord, passado, and bellows-mender, which evoke the language of Shakespearean plays.\n",
        "Rhythm and Meter: Shakespeare often wrote in iambic pentameter, a rhythmic pattern of stressed and unstressed syllables. While the generated text doesn't exactly showcase this meter, it contains rhythmic elements similar to that of Shakespeare's work.\n",
        "\n",
        "\n",
        "**Characters and Themes:** The text contains classic characters from the works of Shakespeare and they all seem to have the same character traits and personalities as they do in real Shakespeare. Characters such as Romeo, Hamlet, and Macbeth constantly mention themes of death, love, power, or make sly sexual innuendos.\n",
        "\n",
        "    “in the morning’s eyeing blessed my love’s tongue, bring him silently”\n",
        "\n",
        "    “Here’s my father and rest, but all is death, but body’s bones.”\n",
        "\n",
        "    “I will not stay in the siege of loving death.\n",
        "    But let the mind to you; farewell. Now I am alone.”\n",
        "\n",
        "    “They bleed so that I may love thee.”\n",
        "\n",
        "    “And Tybalt’s dead”\n",
        "\n",
        "    “Sir, in my heart I am an oppressor”\n",
        "\n",
        "    “Come, sir, There’s blood upon your brow”\n",
        "\n",
        "    “I should live to be born. my lips that lie look upon within.”\n",
        "\n",
        "    “I had most need? The Queen and his mouth, his finespire, abound for if I can see”\n",
        "\n",
        "**Dramatic Elements:** Shakespearean plays are known for their dramatic aspects. While not fully included in this, the generated text contains elements of drama such as fights. One spot I particularly enjoyed because it made me chuckle is when a fight spontaneously broke out.\n",
        "\n",
        "    NURSE:\n",
        "    She sees, my lord, and I will wear it.\n",
        "\n",
        "    HAMLET:\n",
        "    I am but merry!—\n",
        "    I’ll call up our wisest fair Jepantasion.\n",
        "\n",
        "    POLONIUS:\n",
        "    Have I, And put it to you.\n",
        "\n",
        "    ROMEO:\n",
        "    Come, sir, your passado.\n",
        "\n",
        "    [They fight.]\n",
        "\n",
        "Not only did I find this part particularly funny, but I think it also showcases some of the spontaneity of Shakespeare's writing, as sometimes when reading Shakespeare you’re on the edge of your seat, not always knowing what to expect next.\n",
        "\n",
        "**In summary**, while the generated text may not achieve the exact mastery and skill of Shakespeare's writing, it captures enough elements of his style to evoke a sense of familiarity and with his works. In my opinion this highlights Shakespeare's uniqueness and influence of his works, which continue to inspire and influence writers centuries later.\n",
        "\n",
        "**Honorable mentions** of lines that made me laugh\n",
        "\n",
        "    “So let him dumb head”\n",
        "\n",
        "    “FIRST CLOWN:\n",
        "    [Sings.]\n",
        "    An old maid’s, or their own distracted groves,\n",
        "    The virtue of the thure, Lords, Or if thou know’st mine eyes,\n",
        "    And braggart with my tongue!—But, gentle sweet, you shall hear, go join you, I’ll fa you. Do you notice men?,\n",
        "    I qual night, and left me with rum to say, I saw the other senses,\n",
        "    Or my true knight!”\n",
        "\n",
        "**Please note this results were from my specific generation, your generations may produce varying results**"
      ],
      "metadata": {
        "id": "vULxBLeOL8oz"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}